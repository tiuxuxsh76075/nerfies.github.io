<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ProLLM: Protein Chain-of-Thoughts Enhanced LLM for Protein-Protein Interaction Prediction">
  <meta name="keywords" content="ProLLM, Protein-Protein Interaction, Chain-of-Thought">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ProLLM: Protein Chain-of-Thoughts Enhanced LLM</title>

  <!-- 引入字体和样式 -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <!-- 引入 Font Awesome 和 Academicons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css">
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="#">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>
  </div>
</nav>

<!-- 项目标题部分 -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ProLLM: Protein Chain-of-Thoughts Enhanced LLM</h1>
          
          <!-- 作者名称 -->
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="https://mingyuj666.github.io/">Mingyu Jin</a><sup>1</sup>,</span>
            <span class="author-block"><a href="#">Haochen Xue</a><sup>2</sup>,</span>
            <span class="author-block"><a href="https://zhentingwang.github.io/">Zhenting Wang</a><sup>1</sup>,</span>
            <span class="author-block"><a href="#">Boming Kang</a><sup>3</sup>,</span>
            <span class="author-block"><a href="#">Ruosong Ye</a><sup>1</sup>,</span><br>
            <!-- 从这里开始新行 -->
            <span class="author-block"><a href="#">Kaixiong Zhou</a><sup>4</sup>,</span>
            <span class="author-block"><a href="https://mengnandu.com/">Mengnan Du</a><sup>5</sup>,</span>
            <span class="author-block"><a href="https://yongfeng.me/">Yongfeng Zhang</a><sup>1</sup></span>
          </div>

          <!-- 作者机构 -->
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Rutgers University,</span>
            <span class="author-block"><sup>2</sup>University of Liverpool,</span>
            <span class="author-block"><sup>3</sup>Peking University,</span>
            <span class="author-block"><sup>4</sup>MIT,</span>
            <span class="author-block"><sup>5</sup>New Jersey Institute of Technology</span>
          </div>

          <!-- 新增的 arXiv 和 GitHub 链接按钮，以及 OpenReview 链接 -->
          <div class="link-buttons">
            <span class="link-block">
              <a href="https://arxiv.org/abs/2405.06649" class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv</span>
              </a>
            </span>

            <span class="link-block">
              <a href="https://github.com/MingyuJ666/ProLLM" class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
              </a>
            </span>

            <!-- 新增的 OpenReview 图片按钮 -->
            <span class="link-block">
              <a href="https://openreview.net/group?id=colmweb.org/COLM/2024/Conference/Authors&referrer=%5BHomepage%5D(%2F)" class="external-link button is-normal is-rounded is-dark">
                <span class="icon">  
                    <img src="./static/images/colm.png" alt="Accepted by colm" style="width: 24px; height: 36px;"> <!-- 修改图片高度 -->
                </span>
                <span>Accepted by COLM'24</span>
              </a>
            </span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- 摘要部分 -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The prediction of protein-protein interactions (PPIs) is crucial for understanding biological functions and diseases. 
            We propose ProLLM, a novel framework that employs an LLM tailored for PPI for the first time. Specifically, we propose 
            Protein Chain of Thought (ProCoT), which simulates the biological mechanism of signaling pathways as natural language 
            prompts, predicting interactions between proteins through reasoning. ProLLM significantly improves prediction accuracy 
            and generalizability compared to existing methods.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- 插入图片：ProLLM框架图，来源PDF Figure 1 -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">ProLLM Framework</h2>
        <div class="content has-text-justified">
          <p>
            Our framework, ProLLM, employs a Protein Chain of Thought (ProCoT) approach to simulate protein signaling pathways 
            as natural language prompts. It incorporates embedding replacement with ProtTrans vectors and instruction fine-tuning 
            on protein knowledge datasets, leading to a deeper understanding of complex biological problems.
          </p>
        </div>
        <figure>
           <img src="./static/images/Fig1.png" alt="ProLLM Framework">
           <figcaption>Figure 1: ProLLM Framework</figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<!-- 相关工作 -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Motivation</h2>
        <div class="content has-text-justified">
          <p>
            Previous approaches like CNNs and GNNs focus on direct physical interactions but fail to capture non-physical connections. 
            LLMs offer a new avenue by representing protein-protein interactions as natural language problems. We build on methods 
            such as ProtBERT and ProteinLM, enhancing the model's capability to reason about protein chains through ProCoT.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- 插入图片：对比现有方法的图，来源PDF Figure 2 -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        
         <img src="./static/images/compare.png" alt="Comparison with Existing Methods"> 
      </div>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Methods</h2>
        <div class="content has-text-justified">
          <p>
            The following sections describe the detailed methodology employed in ProLLM for protein-protein interaction prediction. This framework is divided into four key sectors, each contributing to the overall performance improvement in predicting protein interactions.
          </p>
        </div>

        <!-- Sector 1: Protein data to natural language in ProCoT format -->
        <div class="content has-text-justified">
          <h3 class="title is-4">Sector 1: Protein Data to Natural Language in ProCoT Format</h3>
          <p>
            In the first step of our methodology, we transform raw protein data into natural language prompts in the Protein Chain of Thought (ProCoT) format. This process simulates the biological signaling pathways between proteins, using descriptions of actions such as "activate", "inhibit", and "bind" to represent the interactions. By converting these structured data into a natural language format, we enable the large language model (LLM) to process and reason about the relationships between proteins step by step.
          </p>
          <p>
           For example, a typical ProCoT prompt might be: "&lt;Protein_A&gt; acts as a receptor protein, activate &lt;Protein_B&gt;. &lt;Protein_B&gt; acts as a signaling protein, inhibit &lt;Protein_C&gt;. &lt;Protein_C&gt; acts as an effector protein, catalysis &lt;Protein_D&gt;. What is the relationship between &lt;Protein_A&gt; and &lt;Protein_D&gt;?" This helps the model simulate the biological reasoning chain, leading to more accurate interaction predictions.
          </p>
        </div>

        <!-- Sector 2: Replace with ProtTrans Embedding -->
        <div class="content has-text-justified">
          <h3 class="title is-4">Sector 2: Replace with ProtTrans Embedding</h3>
          <p>
            In this step, we enhance the LLM’s understanding of protein sequences by replacing standard language embeddings with protein-specific embeddings generated from the ProtTrans model. ProtTrans, a large-scale pre-trained model, generates embeddings that capture the biophysical and structural properties of proteins.
          </p>
          <p>
            By integrating ProtTrans embeddings directly into the LLM, we provide it with richer biological information, allowing it to better understand the relationships between proteins. This step is crucial for improving the model’s performance in recognizing complex protein interactions beyond simple textual descriptions.
          </p>
        </div>

        <!-- Sector 3: Instruction Fine-tuning on ProLLM -->
        <div class="content has-text-justified">
          <h3 class="title is-4">Sector 3: Instruction Fine-tuning on ProLLM</h3>
          <p>
            After embedding replacement, we fine-tune the ProLLM on a protein knowledge dataset using an instruction-based approach. This fine-tuning process allows the model to gain specialized knowledge about proteins, including their functions and biological processes. We use instruction datasets like Mol-Instructions, which provide detailed prompts and corresponding answers about protein functions.
          </p>
          <p>
            For instance, a fine-tuning instruction might ask: "Based on the following protein sequence, predict its function." The model is then able to infer important details about the protein’s role in various biological processes, further enhancing its predictive power when applied to new protein-protein interaction tasks.
          </p>
        </div>

        <!-- Sector 4: Train on ProCoT Format Dataset -->
        <div class="content has-text-justified">
          <h3 class="title is-4">Sector 4: Train on ProCoT Format Dataset</h3>
          <p>
            In the final step, we train the ProLLM on datasets formatted in the ProCoT structure. The dataset contains complex signaling pathways between proteins, where the model learns to predict the type of interaction (e.g., activation, inhibition) between two proteins.
          </p>
          <p>
            During training, the model is presented with input questions such as: "<Protein_A> activates <Protein_B>, which inhibits <Protein_C>. What is the relationship between <Protein_A> and <Protein_D>?" The model’s output is then compared to the real answer, and the loss is calculated to fine-tune the predictions. This process iteratively improves the model’s accuracy in predicting protein-protein interactions.
          </p>
        </div>

        <!-- 插入图片 -->
        <figure>
           <img src="./static/images/main.png" alt="ProLLM Methodology">
           <figcaption>Figure: Overview of the ProLLM Framework and Methodology</figcaption>
        </figure>
        
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiments</h2>
        
        <!-- Comparative Experiment -->
        <div class="content has-text-justified">
          <h3 class="title is-4">Comparative Experiment</h3>
          <p>
            We conducted extensive experiments to evaluate the performance of ProLLM compared to existing models. The comparison includes models based on Convolutional Neural Networks (CNNs), Graph Neural Networks (GNNs), and other Large Language Models (LLMs). The results were evaluated on several widely used Protein-Protein Interaction (PPI) datasets, including Human, SHS27K, SHS148K, and STRING.
          </p>
          <p>
            ProLLM outperforms existing models in terms of both accuracy and generalizability. The key advantage of ProLLM lies in its ability to model complex protein signaling pathways through the ProCoT format, which allows it to capture indirect interactions between proteins more effectively than traditional models.
          </p>
        </div>

        <!-- Insert Comparison Table or Image -->
        <figure>
           <img src="./static/images/compare_exp.png" alt="Comparison of Model Performance">
           <figcaption>Figure: Performance Comparison between ProLLM and Baseline Methods on Various Datasets</figcaption>
        </figure>

        <!-- Comparative Results Explanation -->
        <div class="content has-text-justified">
          <p>
            The comparison results show that ProLLM achieves significantly higher micro-F1 scores across all datasets. For example, on the Human dataset, ProLLM achieved a micro-F1 score of 91.05, surpassing GNN-PPI's score of 78.61 and CNN-based methods like DPPI's score of 54.19. Similar improvements were observed in SHS27K, SHS148K, and STRING datasets, where ProLLM consistently demonstrated better performance due to its ability to model non-physical connections between proteins.
          </p>
          <p>
            The following table and figure summarize the comparative performance of different models, highlighting ProLLM's superiority in both pre-trained and non-pre-trained setups.
          </p>
        </div>
        
      </div>
    </div>
  </div>
</section>

<!-- 消融实验部分 -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4">Ablation Study</h3>
        
        <!-- Ablation Study Description -->
        <div class="content has-text-justified">
          <p>
            To better understand the contribution of each component in ProLLM, we conducted an ablation study. The ablation experiments were performed by systematically removing or altering key components of the model to assess their impact on overall performance. The components studied include:
          </p>
          <ul>
            <li><strong>ProCoT format:</strong> Removing or shuffling the ProCoT data disrupts the biological reasoning chain, which affects the model's ability to simulate signaling pathways.</li>
            <li><strong>Embedding Replacement:</strong> Removing the protein-specific embeddings from ProtTrans reduces the model’s capacity to understand protein sequences, impacting the prediction accuracy.</li>
            <li><strong>Instruction Fine-tuning:</strong> Omitting the instruction-based fine-tuning on protein knowledge datasets limits the model’s knowledge about biological functions, leading to less accurate predictions.</li>
          </ul>
        </div>

        <!-- Insert Ablation Results Image -->
        <figure>
           <img src="./static/images/abalation.png" alt="Ablation Study Results">
           <figcaption>Figure: Ablation Study Results on Various Datasets</figcaption>
        </figure>

        <!-- Ablation Study Results Explanation -->
        <div class="content has-text-justified">
          <p>
            The results of the ablation study, as shown in the figure, demonstrate the importance of each component in ProLLM's overall performance. Removing the ProCoT format resulted in the largest decrease in performance, as the model was no longer able to reason about signaling pathways. Without ProtTrans embeddings, the model’s understanding of protein sequences was significantly reduced, resulting in lower micro-F1 scores across all datasets. Lastly, the removal of instruction fine-tuning also caused a notable drop in performance, particularly in tasks that required understanding protein functions.
          </p>
          <p>
            These findings highlight the critical role each component plays in enhancing the predictive capabilities of ProLLM, particularly the ProCoT format and embedding replacement with ProtTrans.
          </p>
        </div>
        
      </div>
    </div>
  </div>
</section>








              
<!-- 结论部分 -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Conclusions</h2>
        <div class="content has-text-justified">
          <p>
            In conclusion, ProLLM is a powerful framework for predicting protein-protein interactions. It leverages the power of LLMs 
            to represent signaling pathways in a natural language format, significantly enhancing accuracy and generalizability.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- 致谢部分 -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-4">Acknowledgements</h3>
        
        <!-- Acknowledgements Content -->
        <div class="content has-text-justified">
          <p>
            We would like to express our gratitude to all those who contributed to this project. Special thanks to Dr. Wenyue Hua, Dr. Kai Mei, and Dr. Taowen Wang for their insightful discussions and suggestions during the development of this work. We also acknowledge the support from our institutions, including Rutgers University, the University of Liverpool, Peking University, MIT, and the New Jersey Institute of Technology, for providing the necessary resources and computational infrastructure.
          </p>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{jin2024prollm,
  title={ProLLM: Protein Chain-of-Thoughts Enhanced LLM for Protein-Protein Interaction Prediction},
  author={Jin, Mingyu and Xue, Haochen and Wang, Zhenting and Kang, Boming and Ye, Ruosong and Zhou, Kaixiong and Du, Mengnan and Zhang, Yongfeng},
  journal={bioRxiv},
  pages={2024--04},
  year={2024},
  publisher={Cold Spring Harbor Laboratory}
}</code></pre>
  </div>
</section>

              
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>This project is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.</p>
    </div>
  </div>
</footer>

</body>
</html>
