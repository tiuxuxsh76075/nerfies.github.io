<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ProLLM: Protein Chain-of-Thoughts Enhanced LLM for Protein-Protein Interaction Prediction">
  <meta name="keywords" content="ProLLM, Protein-Protein Interaction, Chain-of-Thought">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ProLLM: Protein Chain-of-Thoughts Enhanced LLM</title>

  <!-- 引入字体和样式 -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="#">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>
  </div>
</nav>

<!-- 项目标题部分 -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ProLLM: Protein Chain-of-Thoughts Enhanced LLM</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><a href="#">Mingyu Jin</a><sup>1</sup>,</span>
            <span class="author-block"><a href="#">Haochen Xue</a><sup>2</sup>,</span>
            <span class="author-block"><a href="#">Zhenting Wang</a><sup>1</sup>,</span>
            <span class="author-block"><a href="#">Boming Kang</a><sup>3</sup>,</span>
            <span class="author-block"><a href="#">Ruosong Ye</a><sup>1</sup>,</span>
            <span class="author-block"><a href="#">Kaixiong Zhou</a><sup>4</sup>,</span>
            <span class="author-block"><a href="#">Mengnan Du</a><sup>5</sup>,</span>
            <span class="author-block"><a href="#">Yongfeng Zhang</a><sup>1</sup></span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Rutgers University,</span>
            <span class="author-block"><sup>2</sup>University of Liverpool,</span>
            <span class="author-block"><sup>3</sup>Peking University,</span>
            <span class="author-block"><sup>4</sup>MIT,</span>
            <span class="author-block"><sup>5</sup>New Jersey Institute of Technology</span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Background</h2>
        <div class="content has-text-justified">
          <p>
            Protein-protein interactions (PPIs) are essential for understanding biological functions and are crucial for biomedical and pharmaceutical research. 
            However, predicting these interactions accurately is challenging due to the complex nature of biological signaling pathways. Experimental methods 
            are often time-consuming, prompting the need for computational solutions like ProLLM.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- 摘要部分 -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The prediction of protein-protein interactions (PPIs) is crucial for understanding biological functions and diseases. 
            We propose ProLLM, a novel framework that employs an LLM tailored for PPI for the first time. Specifically, we propose 
            Protein Chain of Thought (ProCoT), which simulates the biological mechanism of signaling pathways as natural language 
            prompts, predicting interactions between proteins through reasoning. ProLLM significantly improves prediction accuracy 
            and generalizability compared to existing methods.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- 插入图片：ProLLM框架图，来源PDF Figure 1 -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">ProLLM Framework</h2>
        <div class="content has-text-justified">
          <p>
            Our framework, ProLLM, employs a Protein Chain of Thought (ProCoT) approach to simulate protein signaling pathways 
            as natural language prompts. It incorporates embedding replacement with ProtTrans vectors and instruction fine-tuning 
            on protein knowledge datasets, leading to a deeper understanding of complex biological problems.
          </p>
        </div>
        <figure>
           <img src="./static/images/Fig1.png" alt="ProLLM Framework">
           <figcaption>Figure 1: ProLLM Framework</figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<!-- 相关工作 -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Motivation</h2>
        <div class="content has-text-justified">
          <p>
            Previous approaches like CNNs and GNNs focus on direct physical interactions but fail to capture non-physical connections. 
            LLMs offer a new avenue by representing protein-protein interactions as natural language problems. We build on methods 
            such as ProtBERT and ProteinLM, enhancing the model's capability to reason about protein chains through ProCoT.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- 插入图片：对比现有方法的图，来源PDF Figure 2 -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        
         <img src="./static/images/compare.png" alt="Comparison with Existing Methods"> 
      </div>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Methods</h2>
        <div class="content has-text-justified">
          <p>
            The following sections describe the detailed methodology employed in ProLLM for protein-protein interaction prediction. This framework is divided into four key sectors, each contributing to the overall performance improvement in predicting protein interactions.
          </p>
        </div>

        <!-- Sector 1: Protein data to natural language in ProCoT format -->
        <div class="content has-text-justified">
          <h3 class="title is-4">Sector 1: Protein Data to Natural Language in ProCoT Format</h3>
          <p>
            In the first step of our methodology, we transform raw protein data into natural language prompts in the Protein Chain of Thought (ProCoT) format. This process simulates the biological signaling pathways between proteins, using descriptions of actions such as "activate", "inhibit", and "bind" to represent the interactions. By converting these structured data into a natural language format, we enable the large language model (LLM) to process and reason about the relationships between proteins step by step.
          </p>
          <p>
            For example, a typical ProCoT prompt might be: "<Protein_A> acts as a receptor protein, activate <Protein_B>. <Protein_B> acts as a signaling protein, inhibit <Protein_C>. <Protein_C> acts as an effector protein, catalysis <Protein_D>. What is the relationship between <Protein_A> and <Protein_D>?" This helps the model simulate the biological reasoning chain, leading to more accurate interaction predictions.
          </p>
        </div>

        <!-- Sector 2: Replace with ProtTrans Embedding -->
        <div class="content has-text-justified">
          <h3 class="title is-4">Sector 2: Replace with ProtTrans Embedding</h3>
          <p>
            In this step, we enhance the LLM’s understanding of protein sequences by replacing standard language embeddings with protein-specific embeddings generated from the ProtTrans model. ProtTrans, a large-scale pre-trained model, generates embeddings that capture the biophysical and structural properties of proteins.
          </p>
          <p>
            By integrating ProtTrans embeddings directly into the LLM, we provide it with richer biological information, allowing it to better understand the relationships between proteins. This step is crucial for improving the model’s performance in recognizing complex protein interactions beyond simple textual descriptions.
          </p>
        </div>

        <!-- Sector 3: Instruction Fine-tuning on ProLLM -->
        <div class="content has-text-justified">
          <h3 class="title is-4">Sector 3: Instruction Fine-tuning on ProLLM</h3>
          <p>
            After embedding replacement, we fine-tune the ProLLM on a protein knowledge dataset using an instruction-based approach. This fine-tuning process allows the model to gain specialized knowledge about proteins, including their functions and biological processes. We use instruction datasets like Mol-Instructions, which provide detailed prompts and corresponding answers about protein functions.
          </p>
          <p>
            For instance, a fine-tuning instruction might ask: "Based on the following protein sequence, predict its function." The model is then able to infer important details about the protein’s role in various biological processes, further enhancing its predictive power when applied to new protein-protein interaction tasks.
          </p>
        </div>

        <!-- Sector 4: Train on ProCoT Format Dataset -->
        <div class="content has-text-justified">
          <h3 class="title is-4">Sector 4: Train on ProCoT Format Dataset</h3>
          <p>
            In the final step, we train the ProLLM on datasets formatted in the ProCoT structure. The dataset contains complex signaling pathways between proteins, where the model learns to predict the type of interaction (e.g., activation, inhibition) between two proteins.
          </p>
          <p>
            During training, the model is presented with input questions such as: "<Protein_A> activates <Protein_B>, which inhibits <Protein_C>. What is the relationship between <Protein_A> and <Protein_D>?" The model’s output is then compared to the real answer, and the loss is calculated to fine-tune the predictions. This process iteratively improves the model’s accuracy in predicting protein-protein interactions.
          </p>
        </div>

        <!-- 插入图片 -->
        <figure>
           <img src="./static/images/main.png" alt="ProLLM Methodology">
           <figcaption>Figure: Overview of the ProLLM Framework and Methodology</figcaption>
        </figure>
        
      </div>
    </div>
  </div>
</section>

<!-- 结论部分 -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Conclusions</h2>
        <div class="content has-text-justified">
          <p>
            In conclusion, ProLLM is a powerful framework for predicting protein-protein interactions. It leverages the power of LLMs 
            to represent signaling pathways in a natural language format, significantly enhancing accuracy and generalizability.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>This project is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.</p>
    </div>
  </div>
</footer>

</body>
</html>
